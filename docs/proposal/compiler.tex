\documentclass{article}

\usepackage{listings}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{color}
\usepackage{amsmath}

\lstset{mathescape=true,basicstyle=\small,frame=lines}

\title{cs252r: coq compilation \& runtime}
\author{Daniel Huang \and Gregory Malecha \and Scott Moore}

\newcommand{\todo}[1]{\textcolor{red}{#1}}

\begin{document}

\maketitle

Our project is project expanded to encompass 5 main tasks. The foremost of those is to have a working compiler, i.e. we placed a large emphasis on generating runnable LLVM code. This resulted in five pieces:

\begin{description}
\item[Driver \& Test Tools]
We built (and refactored the previous driver) to run our compiler with a number of options, e.g. outputting intermediate representations, running and not running optimizations, etc. This makes the compiler externally usuable and facilitates testing. In addition, we have extracted an interpreter that can be used to test optimizations. (Section~\ref{sec:driver})
\item[Refactored ILs]
Our initial tasks centered around refactoring the intermediate language. In the Cps representation, we syntactically distinguished continuations and extended the IL to support monadic operations in a world-passing style. In addition, we introduced a new intermediate language (Low) that separates the details of this Cps representation from the details of code-generation. (Section~\ref{sec:il})
\item[Garbage Collection]
GC-based functional languages require a reasonable garbage collector and runtime environment. We developed a Chaney-copying collector built using LLVM's GC intrinsics. (Section~\ref{sec:gc})
\item[Input/Output]
To support debugging, our runtime supports output operations to print ascii characters. These can be composed in Coq to implement displaying strings and other interesting data types. These operations are accessed via parameters exposed in Coq and compiled to special routines by our compiler. (Section~\ref{sec:io})
\item[Abstract Interpretation] 
We built an abstract interpreter for our Cps representation to facilitate more interesting analyses. We have instantiated it to do a standard control-flow analysis though it is phrased in a way that enables it to be used for a variety of analyses. (Section~\ref{sec:abstract-interp})
\item[Destructive Updates]
A major issue with functional languages is the creation of temporary data structures. For example, threading a state through computations in the state monad. To avoid the cost of many small allocations (primarily to ease the GC burden since allocation is reasonable fast) we have an optimization that uses the result of static analysis to reuse dead memory. (Section~\ref{sec:destructive-updates})
\item[Contification]
Distinguished continutations are handled efficiently by our compiler enabling us to stack allocate data associated with continuations. An optimization works on demoting first-class functions into continuations to increase the applicability of this process. (Section~\ref{sec:contification})
\end{description}

\section{The Driver}
\label{sec:driver}

The driver is built from using standard Coq extraction to ML to provide an efficient way to run the top-level compiler. For example, we can compile and run the standard HelloWorld program using the following command line:

\begin{verbatim}
$ ./Wrapper.byte - ...
$ ./helloworld.native
Hello World!
\end{verbatim}

In in addition to the driver, we also have an interpreter for our CpsK language. The semantics are faithful to the real semantics with the exception that there is no semantic distinction between continuations and closures. The former should be not be allowed to escape. We rely on our syntactic distinction to enforce this though it may be possible to break this requirement. The interpreter can be run using:

\begin{verbatim}
$  (even 10)
$
....
\end{verbatim}

\section{Intermediate Languages}
\label{sec:il}

Our compiler is based on two core intermediate languages, CpsK and Low, both shown in Figure~\ref{fig:cpsk-low}.
\begin{figure}

TODO

\caption{}
\label{fig:cpsk-low}
\end{figure}

The motivation for Low is to provide a higher-level, but imperative language to serve as an intermediate point between CpsK and LLVM. This enables the final code generation phase to be simpler and focus more on garbage collection while avoiding the need to do large transformations to the structure of code.

Low has several interesting design decisions:
\begin{description}
\item[Functional Blocks]
\item[Explicit Memory Operations]
\item[Inline Calls]
\end{description}

\section{Input/Output}
\label{sec:io}

The compiler provides a small runtime library that enables compilation of computations that perform I/O. These operations are exposed to Coq using a simple IO mond that supports the standard \lstinline!return! and \lstinline!bind! in addition to \lstinline!printChar : ascii -> IO unit!. Extraction rules are specified for these functions and, if the program is compiled with I/O support, these functions are included by the compiler.

\lstinline!printChar! is implemented as a monadic operation in CpsK. \todo{two options for implementation}


\section{Garbage Collection}
\label{sec:gc}

The first phase of the project will be to implement a simple semispace collector \`a la Cheney's algorithm. Part of this initial effort will be abstracting the code generation phase in the Coq part of the compiler to allow more flexibility during the LLVM codegen and runtime.

The second phase of the project will be to implement a more sophisticated garbage collection algorithm. This algorithm could be \emph{concurrent} or \emph{parallel} or both. A concurrent collector would be appealing to gain parallelism even though our underlying language is sequential (at least our current implementation). Even in a non-concurrent setting, a parallel collector would improve our throughput and latency. In either case, we would probably use two generations, reusing the semispace collector for the young generation.

A quick literature search shows there is some existing work on implementing concurrent or parallel collectors for functional langauges:
\begin{itemize}
\item In Haskell: ``Parallel Generational-Copying Garbage Collection with a Block-Structured Heap''~\cite{marlow:parallel}
\item In ML: ``Very Concurrent Mark-\&-Sweep Garbage Collection without Fine-Grain Synchronization''~\cite{Huelsbergen}
\item ``The Garbage Collection Handbook''~\cite{gcbook} provides a comprehensive overview of possible approaches and implementation issues.
\end{itemize}

My hope is that some of these implementation issues are mitigated by our purely-functional setting.


\section{Abstract Interpretation}
\label{sec:abstract-interp}

We implemented a Horn-style ~\cite{van2010abstracting} abstract interpreter for the CpsK language. We parameterize it over an abstract domain so it can be instantiated different domains to perform different analyses. We show the abstract domain in Figure~\ref{fig:absinterp}.

\begin{figure}
\centering
\begin{minipage}[t]{.2\textwidth}
\begin{align*}
C: & \text{Context} \\
PP: & \text{Program point} \\
\hat{V}: & \text{Abstract values} \\
D: & \text{Domain}
\end{align*}
\end{minipage}
\hspace{20pt}
\begin{minipage}[t]{.43\textwidth}
\begin{lstlisting}
Class AbsDomain : Type :=
{ lookup  : C -> PP -> D -> V
; update  : C -> PP -> V -> D -> D
; joinA   : D -> D -> D
; bottomA : V 
; topA    : V 
; dom_leq : D -> D -> bool
}.
\end{lstlisting}
\end{minipage}
\caption{Select parameters of the abstract interpreter.}
\label{fig:absinterp}
\end{figure}

\begin{figure}
\begin{align*}
& p \in PP, i \in Z, c \in \text{str}, k \in \text{cont}, x \in \text{var}, e \in \text{exp}, \ell \in \text{loc} \\
C := & \;\; \text{unit} \\
PP := & \;\; \text{var} + \text{cont} \\
\hat{v} ::= & \;\; \texttt{Int}(i) \;\; | \;\; \texttt{Constructor}(c) \;\; | \;\; \texttt{Loc}(\ell) \;\; | \;\; \texttt{Closure}(p, \lambda \vec{k} \vec{x}. e) \\
\hat{V}::= & \;\; \texttt{Any} \;\; | \;\; [\hat{v}_1, \hdots, \hat{v}_n] \\
D: & \;\; \{{\tt env}: PP \rightarrow \hat{V} L ; \;\;\; {\tt heap}: \text{loc} \rightarrow [\hat{V}_1, \hdots, \hat{V}_n] \}
\end{align*}
\caption{Instantiation of abstract domain for 0-CFA.}
\label{fig:cfa0}
\end{figure}
For 0-CFA for our CpsK language (Figure~\ref{fig:cfa0}), we can treat program points as either a variable or continuation since they are uniquely named. Furthermore, we can treat abstract locations as just a variable name. 

\section{Destructive Updates}
\label{sec:destructive-updates}

The main idea of a destructive update is to reuse previously allocated space instead of allocating new space if we can prove that all pointers to the old space are dead. One interesting application of this optimization is for programs that use the state monad. After an inlining pass inlines the definitions of bind to expose the state tuples explicitly, we can destructively update state that is threaded through the program instead of allocating new space for each bind. We based our optimization off the ideas described by Dimoulas and Wand in ~\cite{dimoulas2009higher}, which presents an algorithm for detecting and transforming higher-order programs to use destructive updates. We summarize the optimization below. 

\begin{itemize}
\item Perform control flow analysis ${\tt cfa} : {\rm CpsK.exp} \rightarrow (PP \rightarrow \mathcal{P}(\hat{V}))$ using abstract interpretation. 
\item Perform reachability analysis ${\tt reach} : (PP \rightarrow \mathcal{P}(\hat{v})) \rightarrow (PP \rightarrow {\rm set} \; PP)$ using the result of the CFA, to compute for each program point the set of program points that flow to it (i.e., transitive closure on the domain produced by CFA).
\item Perform liveness analysis ${\tt live} : {\rm CpsK.exp} \rightarrow (PP \rightarrow {\rm set} \; PP) \rightarrow (PP \rightarrow {\rm set} \; PP)$. We use the reachability analysis to compute for a given program point, which variables reach definitions in the rest of the expression to determine which variables are still live. 
\item Transform program ${\tt dupate} : {\rm CpsK.exp} \rightarrow (PP \rightarrow {\rm set} \; PP) \rightarrow {\rm Low.program}$. During lowering from a CpsK program to a Low program, we keep around a set of tuple variables in scope. When we encounter a {\tt MkTuple} operation, we check to see if there is a tuple variable in scope that is dead. If so, we perform a destructive update by reusing to the dead variable instead of calling {\tt malloc}. Figures ~\ref{fig:dupdate1} ~\ref{fig:dupdate2} ~\ref{fig:dupdate3} demonstrate the results of running our optimization.
\end{itemize}

\begin{figure}
\centering
\begin{minipage}[t]{.4\textwidth}
\begin{lstlisting}
let x = MkTuple('S', '0') in
let y = MkTuple('S', '0') in
let z = MkTuple('S', '0') in
z
\end{lstlisting}
\end{minipage}
\hspace{20pt}
\begin{minipage}[t]{.4\textwidth}
\begin{lstlisting}
coq_main() {
l1(inscope: ; args: ):
  [x_1] = malloc(<int,int>)
  store Con('S')@int into x_1 + 0
  store Con('0')@int into x_1 + 1
  store Con('S')@int into x_1 + 0
  store Con('0')@int into x_1 + 1
  store Con('S')@int into x_1 + 0
  store Con('0')@int into x_1 + 1
  halt x_1
}
\end{lstlisting}
\end{minipage}
\caption{In this example, only the last variable z is
   used. Consequently, we can write y into x, and z into y. }
\label{fig:dupdate1}
\end{figure}

\begin{figure}
\centering
\begin{minipage}[t]{.4\textwidth}
\begin{lstlisting}
let x = MkTuple('S', '0') in
let y = MkTuple('S', '0') in
let w = MkTuple('S', x) in
let z = MkTuple('S', '0') in
z
\end{lstlisting}
\end{minipage}
\hspace{20pt}
\begin{minipage}[t]{.4\textwidth}
\begin{lstlisting}
coq_main() {
l1(inscope: ; args: ):
  [x_1] = malloc(<int,int>)
  store Con('S')@int into x_1 + 0
  store Con('0')@int into x_1 + 1
  [x_2] = malloc(<int,int>)
  store Con('S')@int into x_2 + 0
  store Con('0')@int into x_2 + 1
  store Con('S')@int into x_2 + 0
  store x_1@int into x_2 + 1
  store Con('S')@int into x_2 + 0
  store Con('0')@int into x_2 + 1
  halt x_2
}
\end{lstlisting}
\end{minipage}
\caption{In this example, w can destructively update y since y is
  dead. The variable y could not use x because a future program point
  w uses it. At program point z, w is dead, so z can update w. }
\label{fig:dupdate2}
\end{figure}

\begin{figure}
\centering
\begin{minipage}[t]{.4\textwidth}
\begin{lstlisting}
let x = MkTuple('S', '0') in
let y = MkTuple('S', '0') in
let w = MkTuple('S', x) in
let z = MkTuple('S', '0') in
w
\end{lstlisting}
\end{minipage}
\hspace{20pt}
\begin{minipage}[t]{.4\textwidth}
\begin{lstlisting}
coq_main() {
l1(inscope: ; args: ):
  [x_1] = malloc(<int,int>)
  store Con('S')@int into x_1 + 0
  store Con('0')@int into x_1 + 1
  [x_2] = malloc(<int,int>)
  store Con('S')@int into x_2 + 0
  store Con('0')@int into x_2 + 1
  store Con('S')@int into x_2 + 0
  store x_1@int into x_2 + 1
  [x_4] = malloc(<int,int>)
  store Con('S')@int into x_4 + 0
  store Con('0')@int into x_4 + 1
  halt x_2
}
\end{lstlisting}
\end{minipage}
\caption{In this example, w destructively updates y since y is
  dead. At program point z, we must allocate since both w and x are
  being used, and w is already using y. }
\label{fig:dupdate3}
\end{figure}

%Peforming optimizations for monadic computation will involve two
%parts. First, since our CPS representation does not contain monadic
%constructs, we need to run some sort of inference algorithm to recover
%the monadic structure. For instance, recovering the state monad
%might include running escape analysis to track the flow of function
%arguments into functions that have ``bind-like'' signatures. The
%second part will include the actual optimization once we have 
%recovered the monadic structure. For the state monad, this might
%include rewriting bind such that it does not construct a new state
%with updated values each time, but rather update the original state
%with the new values. 

%I have included a few papers that I think will be useful for me to take a
%look at
%~\cite{ghani2007monadic,pudlak2011reader,tolmach1998optimizing}, and
%would appreciate further suggestions. The
%first~\cite{ghani2007monadic}, talks about how shortcut fusion can be
%used to optimize monadic computations that only use monads to thread
%information through the program (might be useful for state, etc.). The
%second~\cite{pudlak2011reader} explains how to translate code using
%reader monads into lambda expressions that do not contain variables. The
%third~\cite{tolmach1998optimizing} describes an IR with monadic types,
%how to translate a ML-like source language into that IR, and provides
%an inference algorithm for translating terms in the ML-like source
%language into the IR. 

\section{Contification}
\label{sec:contification}


\section{Monadic Computation}
\label{sec:mon-com}


\section{Stack Allocation}
\label{sec:stack-alloc}
The core of the idea was presented in Kennedy~\cite{kennedy07cps}. Rather than storing everything in the heap, we'd like to stack allocate information that is accessed in a statically determinable pattern. The benefit, we hope, is that we avoid a lot of heap allocation and thus garbage collection, esspecially before we have optimizations like un-currying and partial applications that are only passed forward.

The use of LLVM may provide difficulties for this task since LLVM has a built-in allocation stack. When we perform this type of operation, we want to replace the current stack frame with the stack frame of the continuation and then tail call the function that we pass the continuation to but preserving the continuations environment (which just replaced our stack frame). The basic proposal for doing this is shown in Figure~\ref{fig:stack-alloc}. 

\begin{figure}
\begin{tabular}{p{0.44\textwidth} c p{0.44\textwidth}}
\begin{lstlisting}[language=c]
void f(...) { 
  // heap allocate the env
  k_env = $\langle$env$\rangle$ ;
  g(x, k) (** tail call g **)
}
\end{lstlisting}
& \quad
& 
\begin{lstlisting}[language=c]
void trampoline_f(...) {
  // using alloca
  k_env = $\langle$env$\rangle$ ;
  // non-tail call of g
  g(x)
}
void f(...) {
  // tail call g
  trampoline_f(...)
}
\end{lstlisting}
\\
(a) before stack allocation & &
(b) after stack allocation \\ 
\end{tabular}
\caption{Strategy for stack allocation of continuations.}
\label{fig:stack-alloc}
\end{figure}

Two alternatives to this approach are:
\begin{enumerate}
\item manually manage a separate data stack (in the style of XFI) where these operations can be performed explicitly.
\item write LLVM intrinsics that perform explicit stack management and implement code generation for them to get the right behavior.
\end{enumerate}
The former seems like it may be feasible, the later seems like it could require considerable work though might be interesting to understand how these things work.

Other potentially interesting papers:
\begin{itemize}
\item ``Contification using dominators''~\cite{fluet01contification}
\item ``The essence of compiling with continuations''~\cite{flanagan04essence}
\item ``Local CPS conversion in a direct-style compiler''~\cite{reppy01local}
\item ``Optimizing nested loops using local CPS conversion''~\cite{reppy02optimizing}
\item ``A Lambda Term Representation Inspired by Linear Ordered Logic''~\cite{abel11lambda}
\end{itemize}

\section{Garbage Collection}
\label{sec:gc}

\bibstyle{natbib}

\bibliography{bib}

\end{document}
